{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem set up: \n",
    "import numpy as np\n",
    "states = ['1','2','3','4','5'.'6','7','8','9','10','11','12','13','14']\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "rewards = [[-1, -1, 0, -1], \n",
    "           [-1, -1, -1, -1], \n",
    "           [-1, -1, -1, -1], \n",
    "           [0, -1, -1, -1],  \n",
    "           [-1, -1, -1, -1], \n",
    "           [-1, -1, -1, -1], \n",
    "           [-1, -1, -1, -1],  \n",
    "           [-1, -1, -1, -1], \n",
    "           [-1, -1, -1, -1],  \n",
    "           [-1, -1, -1, -1], \n",
    "           [-1, 0, -1, -1], \n",
    "           [-1, -1, -1, -1], \n",
    "           [-1, -1, -1, -1], \n",
    "           [-1, -1, -1, 0], ]\n",
    "\n",
    "# states, possibilities = transition[curr_state][action]\n",
    "# states are numbered by the index of \"states\" list defined above.\n",
    "# for each state (row), the probability of executing each action (column)\n",
    "transition = [[([1],[1]), ([2],[1]), ([3],[1])], [([7],[1]), ([4],[1]), ()], \n",
    "              [([4, 7],[0.5, 0.5]), ([4],[1]), ([5],[1])],\n",
    "              [([5, 8],[0.5, 0.5]), ([5],[1]), ()], \n",
    "              [([6],[1]), ([7],[1]), ([8],[1])], \n",
    "              [([9],[1]), ([8],[1]), ()],\n",
    "              [([10],[1]), ([10],[1]),([10],[1]),([10],[1])], \n",
    "              [([10],[1]), ([10],[1]),([10],[1]),([10],[1])],\n",
    "              [([10],[1]), ([10],[1]),([10],[1]),([10],[1])], \n",
    "              [([10],[1]), ([10],[1]),([10],[1]),([10],[1])]]\n",
    "startState = 0\n",
    "endState = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def get_actions(state):\n",
    "    act = [a for a in range(len(actions)) if not np.isnan(rewards[state][a])]\n",
    "    return act\n",
    "\n",
    "def is_end(state):\n",
    "    return state == endState\n",
    "\n",
    "def get_next_state(curr_state,action):\n",
    "    states, poss = transition[curr_state][action]\n",
    "    if len(states) == 1:\n",
    "        return states[0]\n",
    "    else:\n",
    "        return np.random.choice(states, p = poss)\n",
    "\n",
    "def get_reward(curr_state,action):\n",
    "    return rewards[curr_state][action]\n",
    "\n",
    "def random_act(state):\n",
    "    act = get_actions(state)\n",
    "    return np.random.choice(act)\n",
    "        \n",
    "def one_episode(startState):\n",
    "    s_curr = startState\n",
    "    sequence = ''\n",
    "    tot_rewards = 0\n",
    "    while is_end(s_curr) == False:\n",
    "        act = random_act(s_curr)\n",
    "        r = get_reward(s_curr,act)\n",
    "        tot_rewards += r\n",
    "        curr_s = 'state '+str(states[s_curr])+', action '+str(actions[act])+', reward '+str(tot_rewards)+'\\n'\n",
    "        sequence += curr_s\n",
    "        s_next = get_next_state(s_curr,act)\n",
    "        s_curr = s_next\n",
    "    print(sequence + str(states[-1])+'\\n')\n",
    "    return tot_rewards\n",
    "\n",
    "def policy_evaluation(policy):\n",
    "    state_values = np.zeros(len(states))\n",
    "    thresh = 0.001\n",
    "    delta = 999\n",
    "    iteration = 0\n",
    "    while delta > thresh:\n",
    "        iteration += 1\n",
    "        delta = 0\n",
    "        for s in range(len(states)-1):\n",
    "            v_prev = state_values[s]\n",
    "            action, probability = policy[s]\n",
    "            v = 0\n",
    "            for a, p in zip(action,probability):\n",
    "                rewards = get_reward(s, a)\n",
    "                s_next, pp = transition[s][a]\n",
    "                for ns,n_p in zip(s_next,pp):\n",
    "                    rewards += state_values[ns] * n_p\n",
    "                v += rewards * p\n",
    "            state_values[s] = v\n",
    "            delta = max(delta,abs(state_values[s]-v_prev))\n",
    "#         print('Iteration '+str(iteration)+ ', delta = '+str(delta))\n",
    "    return state_values\n",
    "\n",
    "def policy_iteration(initial_policy):\n",
    "    policy = initial_policy\n",
    "    stable = False\n",
    "    iteration = 0\n",
    "    while not stable:\n",
    "        print('\\nWhile iteration '+str(iteration))\n",
    "        state_values = policy_evaluation(policy)\n",
    "        print_state_values(state_values)\n",
    "        \n",
    "        # policy improvement lecture 14 page 16\n",
    "        stable = True\n",
    "        for s in range(len(states)-1):\n",
    "            old_action = policy[s]\n",
    "            possible_acts = get_actions(s)\n",
    "            \n",
    "            # maximize policy value\n",
    "            best_action = None\n",
    "            best_value = -999.0\n",
    "            for a in possible_acts:\n",
    "                v = get_reward(s, a)\n",
    "                s_next, p_next = transition[s][a]\n",
    "                for n_s,n_p in zip(s_next,p_next):\n",
    "                    v += state_values[n_s] * n_p\n",
    "                if v > best_value:\n",
    "                    best_value = v\n",
    "                    best_action = a\n",
    "            policy[s] = ([best_action],[1])\n",
    "            \n",
    "            # stopping criterion\n",
    "            if old_action != policy[s]:\n",
    "                stable = False\n",
    "        iteration += 1\n",
    "        print_policy(policy)\n",
    "    return policy\n",
    "\n",
    "def print_policy(policy):\n",
    "    string = ''\n",
    "    for i in range(len(best_policy)):\n",
    "        a,_ = best_policy[i]\n",
    "        string += str(states[i]) + ': ' + actions[a[0]]+' | '\n",
    "    print('Policy: '+string)\n",
    "    return\n",
    "    \n",
    "def print_state_values(state_values):\n",
    "    string = ''\n",
    "    for i in range(len(state_values)):\n",
    "        string  += str(states[i])+': '+str(state_values[i])+' | '\n",
    "    print('State values: '+string)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1\n",
      "state RU8p, action R, reward 0\n",
      "state RU10p, action P, reward 2\n",
      "state RU8a, action R, reward 2\n",
      "state RU10a, action R, reward 2\n",
      "11am class begins\n",
      "\n",
      "episode 2\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action P, reward 4\n",
      "state RU10a, action S, reward 4\n",
      "11am class begins\n",
      "\n",
      "episode 3\n",
      "state RU8p, action R, reward 0\n",
      "state RU10p, action P, reward 2\n",
      "state RU8a, action R, reward 2\n",
      "state RU10a, action S, reward 2\n",
      "11am class begins\n",
      "\n",
      "episode 4\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action P, reward 4\n",
      "state RU10a, action S, reward 4\n",
      "11am class begins\n",
      "\n",
      "episode 5\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action P, reward 4\n",
      "state RU10a, action R, reward 4\n",
      "11am class begins\n",
      "\n",
      "episode 6\n",
      "state RU8p, action S, reward -1\n",
      "state RD10p, action R, reward -1\n",
      "state RD8a, action R, reward -1\n",
      "state RD10a, action S, reward 3\n",
      "11am class begins\n",
      "\n",
      "episode 7\n",
      "state RU8p, action R, reward 0\n",
      "state RU10p, action P, reward 2\n",
      "state RU8a, action S, reward 1\n",
      "state RD10a, action R, reward 5\n",
      "11am class begins\n",
      "\n",
      "episode 8\n",
      "state RU8p, action R, reward 0\n",
      "state RU10p, action S, reward -1\n",
      "state RD8a, action R, reward -1\n",
      "state RD10a, action P, reward 3\n",
      "11am class begins\n",
      "\n",
      "episode 9\n",
      "state RU8p, action R, reward 0\n",
      "state RU10p, action P, reward 2\n",
      "state RU10a, action R, reward 2\n",
      "11am class begins\n",
      "\n",
      "episode 10\n",
      "state RU8p, action R, reward 0\n",
      "state RU10p, action R, reward 0\n",
      "state RU8a, action S, reward -1\n",
      "state RD10a, action R, reward 3\n",
      "11am class begins\n",
      "\n",
      "episode 11\n",
      "state RU8p, action R, reward 0\n",
      "state RU10p, action P, reward 2\n",
      "state RU8a, action P, reward 4\n",
      "state TU10a, action S, reward 3\n",
      "11am class begins\n",
      "\n",
      "episode 12\n",
      "state RU8p, action R, reward 0\n",
      "state RU10p, action R, reward 0\n",
      "state RU8a, action S, reward -1\n",
      "state RD10a, action R, reward 3\n",
      "11am class begins\n",
      "\n",
      "episode 13\n",
      "state RU8p, action S, reward -1\n",
      "state RD10p, action P, reward 1\n",
      "state RD8a, action R, reward 1\n",
      "state RD10a, action S, reward 5\n",
      "11am class begins\n",
      "\n",
      "episode 14\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action R, reward 2\n",
      "state RU8a, action R, reward 2\n",
      "state RU10a, action R, reward 2\n",
      "11am class begins\n",
      "\n",
      "episode 15\n",
      "state RU8p, action S, reward -1\n",
      "state RD10p, action R, reward -1\n",
      "state RD8a, action P, reward 1\n",
      "state TD10a, action R, reward 4\n",
      "11am class begins\n",
      "\n",
      "episode 16\n",
      "state RU8p, action S, reward -1\n",
      "state RD10p, action R, reward -1\n",
      "state RD8a, action R, reward -1\n",
      "state RD10a, action S, reward 3\n",
      "11am class begins\n",
      "\n",
      "episode 17\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action R, reward 2\n",
      "state RU8a, action R, reward 2\n",
      "state RU10a, action R, reward 2\n",
      "11am class begins\n",
      "\n",
      "episode 18\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action P, reward 4\n",
      "state RU10a, action R, reward 4\n",
      "11am class begins\n",
      "\n",
      "episode 19\n",
      "state RU8p, action S, reward -1\n",
      "state RD10p, action R, reward -1\n",
      "state RD8a, action R, reward -1\n",
      "state RD10a, action P, reward 3\n",
      "11am class begins\n",
      "\n",
      "episode 20\n",
      "state RU8p, action S, reward -1\n",
      "state RD10p, action R, reward -1\n",
      "state RD8a, action R, reward -1\n",
      "state RD10a, action R, reward 3\n",
      "11am class begins\n",
      "\n",
      "episode 21\n",
      "state RU8p, action R, reward 0\n",
      "state RU10p, action S, reward -1\n",
      "state RD8a, action R, reward -1\n",
      "state RD10a, action R, reward 3\n",
      "11am class begins\n",
      "\n",
      "episode 22\n",
      "state RU8p, action S, reward -1\n",
      "state RD10p, action P, reward 1\n",
      "state RD10a, action S, reward 5\n",
      "11am class begins\n",
      "\n",
      "episode 23\n",
      "state RU8p, action R, reward 0\n",
      "state RU10p, action S, reward -1\n",
      "state RD8a, action R, reward -1\n",
      "state RD10a, action R, reward 3\n",
      "11am class begins\n",
      "\n",
      "episode 24\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action P, reward 4\n",
      "state RU10a, action S, reward 4\n",
      "11am class begins\n",
      "\n",
      "episode 25\n",
      "state RU8p, action S, reward -1\n",
      "state RD10p, action P, reward 1\n",
      "state RD8a, action R, reward 1\n",
      "state RD10a, action P, reward 5\n",
      "11am class begins\n",
      "\n",
      "episode 26\n",
      "state RU8p, action S, reward -1\n",
      "state RD10p, action R, reward -1\n",
      "state RD8a, action P, reward 1\n",
      "state TD10a, action S, reward 4\n",
      "11am class begins\n",
      "\n",
      "episode 27\n",
      "state RU8p, action S, reward -1\n",
      "state RD10p, action R, reward -1\n",
      "state RD8a, action R, reward -1\n",
      "state RD10a, action S, reward 3\n",
      "11am class begins\n",
      "\n",
      "episode 28\n",
      "state RU8p, action R, reward 0\n",
      "state RU10p, action P, reward 2\n",
      "state RU8a, action P, reward 4\n",
      "state TU10a, action S, reward 3\n",
      "11am class begins\n",
      "\n",
      "episode 29\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action R, reward 2\n",
      "state RU8a, action S, reward 1\n",
      "state RD10a, action S, reward 5\n",
      "11am class begins\n",
      "\n",
      "episode 30\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action P, reward 4\n",
      "state RU10a, action S, reward 4\n",
      "11am class begins\n",
      "\n",
      "episode 31\n",
      "state RU8p, action S, reward -1\n",
      "state RD10p, action P, reward 1\n",
      "state RD8a, action P, reward 3\n",
      "state TD10a, action P, reward 6\n",
      "11am class begins\n",
      "\n",
      "episode 32\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action P, reward 4\n",
      "state RU10a, action R, reward 4\n",
      "11am class begins\n",
      "\n",
      "episode 33\n",
      "state RU8p, action S, reward -1\n",
      "state RD10p, action P, reward 1\n",
      "state RD8a, action P, reward 3\n",
      "state TD10a, action R, reward 6\n",
      "11am class begins\n",
      "\n",
      "episode 34\n",
      "state RU8p, action R, reward 0\n",
      "state RU10p, action R, reward 0\n",
      "state RU8a, action R, reward 0\n",
      "state RU10a, action S, reward 0\n",
      "11am class begins\n",
      "\n",
      "episode 35\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action P, reward 4\n",
      "state RU10a, action S, reward 4\n",
      "11am class begins\n",
      "\n",
      "episode 36\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action P, reward 4\n",
      "state RU10a, action S, reward 4\n",
      "11am class begins\n",
      "\n",
      "episode 37\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action R, reward 2\n",
      "state RU8a, action S, reward 1\n",
      "state RD10a, action P, reward 5\n",
      "11am class begins\n",
      "\n",
      "episode 38\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action R, reward 2\n",
      "state RU8a, action S, reward 1\n",
      "state RD10a, action R, reward 5\n",
      "11am class begins\n",
      "\n",
      "episode 39\n",
      "state RU8p, action S, reward -1\n",
      "state RD10p, action P, reward 1\n",
      "state RD10a, action R, reward 5\n",
      "11am class begins\n",
      "\n",
      "episode 40\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action P, reward 4\n",
      "state RU10a, action S, reward 4\n",
      "11am class begins\n",
      "\n",
      "episode 41\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action P, reward 4\n",
      "state RU10a, action S, reward 4\n",
      "11am class begins\n",
      "\n",
      "episode 42\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action P, reward 4\n",
      "state RU10a, action P, reward 4\n",
      "11am class begins\n",
      "\n",
      "episode 43\n",
      "state RU8p, action R, reward 0\n",
      "state RU10p, action S, reward -1\n",
      "state RD8a, action R, reward -1\n",
      "state RD10a, action S, reward 3\n",
      "11am class begins\n",
      "\n",
      "episode 44\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action P, reward 4\n",
      "state RU10a, action S, reward 4\n",
      "11am class begins\n",
      "\n",
      "episode 45\n",
      "state RU8p, action R, reward 0\n",
      "state RU10p, action P, reward 2\n",
      "state RU10a, action R, reward 2\n",
      "11am class begins\n",
      "\n",
      "episode 46\n",
      "state RU8p, action S, reward -1\n",
      "state RD10p, action R, reward -1\n",
      "state RD8a, action P, reward 1\n",
      "state TD10a, action P, reward 4\n",
      "11am class begins\n",
      "\n",
      "episode 47\n",
      "state RU8p, action R, reward 0\n",
      "state RU10p, action S, reward -1\n",
      "state RD8a, action R, reward -1\n",
      "state RD10a, action S, reward 3\n",
      "11am class begins\n",
      "\n",
      "episode 48\n",
      "state RU8p, action P, reward 2\n",
      "state TU10p, action R, reward 2\n",
      "state RU8a, action P, reward 4\n",
      "state TU10a, action R, reward 3\n",
      "11am class begins\n",
      "\n",
      "episode 49\n",
      "state RU8p, action S, reward -1\n",
      "state RD10p, action R, reward -1\n",
      "state RD8a, action R, reward -1\n",
      "state RD10a, action P, reward 3\n",
      "11am class begins\n",
      "\n",
      "episode 50\n",
      "state RU8p, action R, reward 0\n",
      "state RU10p, action R, reward 0\n",
      "state RU8a, action P, reward 2\n",
      "state TU10a, action S, reward 1\n",
      "11am class begins\n",
      "\n",
      "Average return for 50 episodes is 3.540\n"
     ]
    }
   ],
   "source": [
    "## 3a 1 and 3 The sequences of experience from each episode, including the Return observed in that episode. (pdf report)\n",
    "tot_r = 0\n",
    "for i in range(50):\n",
    "    print('episode '+ str(i+1))\n",
    "    r = one_episode(startState)\n",
    "    tot_r += r\n",
    "    \n",
    "avg_r = tot_r/50\n",
    "print('Average return for 50 episodes is {:0.3f}'.format(avg_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, delta = 4.0\n",
      "Iteration 2, delta = 3.5\n",
      "Iteration 3, delta = 2.625\n",
      "Iteration 4, delta = 1.5972222222222219\n",
      "Iteration 5, delta = 0\n",
      "The value of state RU8p is 3.514\n",
      "The value of state TU10p is 1.667\n",
      "The value of state RU10p is 2.500\n",
      "The value of state RD10p is 5.375\n",
      "The value of state RU8a is 1.333\n",
      "The value of state RD8a is 4.500\n",
      "The value of state TU10a is -1.000\n",
      "The value of state RU10a is 0.000\n",
      "The value of state RD10a is 4.000\n",
      "The value of state TD10a is 3.000\n",
      "The value of state 11am class begins is 0.000\n"
     ]
    }
   ],
   "source": [
    "### 3a 2. The value of each state using Bellman equation\n",
    "all_actions = [get_actions(s) for s in range(len(states) - 1)]\n",
    "policies = [(a, [1/len(a) for _ in range(len(a))]) for a in all_actions]\n",
    "state_values = policy_evaluation(policies)\n",
    "for i in range(len(state_values)):\n",
    "    print('The value of state {} is {:0.3f}'.format(states[i],state_values[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "While iteration 0\n",
      "State values: RU8p: 4.0 | TU10p: 2.0 | RU10p: 2.5 | RD10p: 6.5 | RU8a: 1.0 | RD8a: 5.0 | TU10a: -1.0 | RU10a: 0.0 | RD10a: 4.0 | TD10a: 3.0 | 11am class begins: 0.0 | \n",
      "Policy: RU8p: S | TU10p: R | RU10p: S | RD10p: P | RU8a: S | RD8a: P | TU10a: P | RU10a: P | RD10a: P | TD10a: P | \n",
      "\n",
      "While iteration 1\n",
      "State values: RU8p: 5.5 | TU10p: 2.0 | RU10p: 4.0 | RD10p: 6.5 | RU8a: 3.0 | RD8a: 5.0 | TU10a: -1.0 | RU10a: 0.0 | RD10a: 4.0 | TD10a: 3.0 | 11am class begins: 0.0 | \n",
      "Policy: RU8p: S | TU10p: R | RU10p: S | RD10p: P | RU8a: S | RD8a: P | TU10a: P | RU10a: P | RD10a: P | TD10a: P | \n",
      "\n",
      "While iteration 2\n",
      "State values: RU8p: 5.5 | TU10p: 3.0 | RU10p: 4.0 | RD10p: 6.5 | RU8a: 3.0 | RD8a: 5.0 | TU10a: -1.0 | RU10a: 0.0 | RD10a: 4.0 | TD10a: 3.0 | 11am class begins: 0.0 | \n",
      "Policy: RU8p: S | TU10p: R | RU10p: S | RD10p: P | RU8a: S | RD8a: P | TU10a: P | RU10a: P | RD10a: P | TD10a: P | \n"
     ]
    }
   ],
   "source": [
    "### 3b policy iteration: learn the optimal policy\n",
    "init_policy = [([0], [1]) for s in range(len(states) - 1)]\n",
    "best_policy = policy_iteration(init_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
